{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qWYxQWkyu6c"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "source: [link](https://platform.openai.com/docs/guides/text?api-mode=responses&prompt-templates-examples=simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOfrTgHIdzwF"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQGYhCx1d77e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPUtdxFKy9AX"
      },
      "source": [
        "## A Simple Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlIEy5OYg-ry",
        "outputId": "d3467be1-037d-4476-fef3-8a6a97fff709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "私はあなたが恋しいです。\n"
          ]
        }
      ],
      "source": [
        "# Create a client and request the LLM a response for a given input:\n",
        "\n",
        "openai_python_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "prompt = f\"\"\"Translate the sentence '\n",
        "<sentence>\n",
        "I miss you\n",
        "</sentence>\n",
        "into Japanese, please.\"\"\"\n",
        "\n",
        "response = openai_python_client.responses.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    input=prompt\n",
        ")\n",
        "\n",
        "# use this form of output for the text, because the output array attribute because: \"It is not safe to assume that the model's text output is present at output[0].content[0].text.\"\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPE8O81bfXvk",
        "outputId": "d54aa268-4fe4-4f5f-a54a-a26401c3a3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'openai.types.responses.response.Response'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 'resp_68840d8cdb9c81999b59184207384add028cd000c9f733ac',\n",
              " 'created_at': 1753484684.0,\n",
              " 'error': None,\n",
              " 'incomplete_details': None,\n",
              " 'instructions': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'response',\n",
              " 'output': [{'id': 'msg_68840d8dfae48199895c264697997f17028cd000c9f733ac',\n",
              "   'content': [{'annotations': [],\n",
              "     'text': '私はあなたが恋しいです。',\n",
              "     'type': 'output_text',\n",
              "     'logprobs': []}],\n",
              "   'role': 'assistant',\n",
              "   'status': 'completed',\n",
              "   'type': 'message'}],\n",
              " 'parallel_tool_calls': True,\n",
              " 'temperature': 1.0,\n",
              " 'tool_choice': 'auto',\n",
              " 'tools': [],\n",
              " 'top_p': 1.0,\n",
              " 'background': False,\n",
              " 'max_output_tokens': None,\n",
              " 'max_tool_calls': None,\n",
              " 'previous_response_id': None,\n",
              " 'prompt': None,\n",
              " 'reasoning': {'effort': None, 'generate_summary': None, 'summary': None},\n",
              " 'service_tier': 'default',\n",
              " 'status': 'completed',\n",
              " 'text': {'format': {'type': 'text'}},\n",
              " 'top_logprobs': 0,\n",
              " 'truncation': 'disabled',\n",
              " 'usage': {'input_tokens': 26,\n",
              "  'input_tokens_details': {'cached_tokens': 0},\n",
              "  'output_tokens': 13,\n",
              "  'output_tokens_details': {'reasoning_tokens': 0},\n",
              "  'total_tokens': 39},\n",
              " 'user': None,\n",
              " 'prompt_cache_key': None,\n",
              " 'safety_identifier': None,\n",
              " 'store': True}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here we can see a more clear structure of the response from the LLM:\n",
        "print(type(response))\n",
        "json.loads(response.model_dump_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zwflTjzy_am"
      },
      "source": [
        "## Message Roles And Instruction Following\n",
        "\n",
        "Whe you create a model, you can assign different roles for it using the `instructions` API. Those instructions give it the rules of how it should behave, repond, and even the tone.\n",
        "\n",
        "The `insructions` have priority over the `input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tdYqN-1wR6o",
        "outputId": "6482aade-6780-4d71-935b-faf404f5378c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eu sinto sua falta\n"
          ]
        }
      ],
      "source": [
        "instructions = f\"\"\"Translate any given sentence into Poruguese ONLY\n",
        "Example:\n",
        "<sentence>\n",
        "I love you\n",
        "</sentence>\n",
        "\n",
        "<reply>\n",
        "Eu te amo\n",
        "</reply>\n",
        "\"\"\"\n",
        "\n",
        "response = openai_python_client.responses.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    instructions=instructions,\n",
        "    input=prompt\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF6eEDqS1TXa"
      },
      "source": [
        "Obeserve that I had to provide a **one-shot** prompt for my instruction in order to achieve the esired output.\n",
        "\n",
        "In an ongoing chat, the instructions API could not be reused, so we can go around this assinging roles on he input as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k1sHdPcz_Jo",
        "outputId": "378fe30f-9858-4a4d-8c4b-93693c36aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eu sinto sua falta\n"
          ]
        }
      ],
      "source": [
        "instructions = f\"\"\"Translate any given sentence into Poruguese ONLY\n",
        "Example:\n",
        "<sentence>\n",
        "I love you\n",
        "</sentence>\n",
        "\n",
        "<reply>\n",
        "Eu te amo\n",
        "</reply>\n",
        "\"\"\"\n",
        "\n",
        "response = openai_python_client.responses.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    input=[\n",
        "        {\"role\": \"developer\", \"content\": instructions},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYQqZEuW71cC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
